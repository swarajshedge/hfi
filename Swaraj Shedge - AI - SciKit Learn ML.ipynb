{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1_t4V9bnt-hB",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "50fbc8263e8f31674a9919dc666a7a1b",
     "grade": false,
     "grade_id": "cell-e911fa75d4ae6ea9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Assignment 1: Predicting overall The Human Freedom Index\n",
    "\n",
    "This notebook contains a set of exercises that will guide you through the different steps of this assignment. Solutions need to be code-based, _i.e._ hard-coded or manually computed results will not be accepted. Remember to write your solutions to each exercise in the dedicated cells and to not modify the test cells. When you are done completing all the exercises submit this same notebook back to moodle in **.ipynb** format.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "The <a href=\"https://www.cato.org/human-freedom-index/2021 \">Human Freedom Index</a> measures economic freedoms such as the freedom to trade or to use sound money, and it captures the degree to which people are free to enjoy the major freedoms often referred to as civil liberties—freedom of speech, religion, association, and assembly— in the countries in the survey. In addition, it includes indicators on rule of law, crime and violence, freedom of movement, and legal discrimination against same-sex relationships. We also include nine variables pertaining to women-specific freedoms that are found in various categories of the index.\n",
    "\n",
    "<u>Citation</u>\n",
    "\n",
    "Ian Vásquez, Fred McMahon, Ryan Murphy, and Guillermina Sutter Schneider, The Human Freedom Index 2021: A Global Measurement of Personal, Civil, and Economic Freedom (Washington: Cato Institute and the Fraser Institute, 2021).\n",
    "    \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-danger\"><b>Submission deadline:</b> Sunday, January 29th, 23:55</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1gbj1gyT16vl",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "73165086c3e0ea8e4c4050187971bd5e",
     "grade": false,
     "grade_id": "cell-0fcbc57512e78927",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "JdETeU66gS1E",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "037ddc9204ef8e30636e4a2ebf41f852",
     "grade": false,
     "grade_id": "cell-8f995c9cdf882820",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 1</b>\n",
    "\n",
    "Load the Human Freedom Index data from the link: https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv in a DataFrame called ```df```.\n",
    "\n",
    "<br><i>[0.25 points]</i>\n",
    "</div>\n",
    "<div class=\"alert alert-warning\">\n",
    "Do not download the dataset. Instead, read the data directly from the provided link\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "id": "2EZ4PgrhYpcn",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3416f03882927817c5373de161fb4a59",
     "grade": false,
     "grade_id": "ex1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>countries</th>\n",
       "      <th>ISO</th>\n",
       "      <th>region</th>\n",
       "      <th>hf_score</th>\n",
       "      <th>hf_rank</th>\n",
       "      <th>hf_quartile</th>\n",
       "      <th>pf_rol_procedural</th>\n",
       "      <th>pf_rol_civil</th>\n",
       "      <th>pf_rol_criminal</th>\n",
       "      <th>...</th>\n",
       "      <th>ef_regulation_business_adm</th>\n",
       "      <th>ef_regulation_business_bureaucracy</th>\n",
       "      <th>ef_regulation_business_start</th>\n",
       "      <th>ef_regulation_business_bribes</th>\n",
       "      <th>ef_regulation_business_licensing</th>\n",
       "      <th>ef_regulation_business_compliance</th>\n",
       "      <th>ef_regulation_business</th>\n",
       "      <th>ef_regulation</th>\n",
       "      <th>ef_score</th>\n",
       "      <th>ef_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>8.14</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.97</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.26</td>\n",
       "      <td>...</td>\n",
       "      <td>5.65</td>\n",
       "      <td>6.67</td>\n",
       "      <td>9.74</td>\n",
       "      <td>6.24</td>\n",
       "      <td>5.62</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.81</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Middle East &amp; North Africa</td>\n",
       "      <td>5.26</td>\n",
       "      <td>154.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.64</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9.31</td>\n",
       "      <td>2.58</td>\n",
       "      <td>8.77</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.84</td>\n",
       "      <td>4.90</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>6.09</td>\n",
       "      <td>129.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.43</td>\n",
       "      <td>3.60</td>\n",
       "      <td>...</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.44</td>\n",
       "      <td>8.73</td>\n",
       "      <td>4.70</td>\n",
       "      <td>7.92</td>\n",
       "      <td>6.78</td>\n",
       "      <td>5.59</td>\n",
       "      <td>5.97</td>\n",
       "      <td>5.50</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Latin America &amp; the Caribbean</td>\n",
       "      <td>7.38</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.94</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>2.71</td>\n",
       "      <td>5.78</td>\n",
       "      <td>9.58</td>\n",
       "      <td>6.53</td>\n",
       "      <td>5.73</td>\n",
       "      <td>6.51</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.50</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>ARM</td>\n",
       "      <td>Caucasus &amp; Central Asia</td>\n",
       "      <td>8.20</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.86</td>\n",
       "      <td>6.96</td>\n",
       "      <td>9.30</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.82</td>\n",
       "      <td>8.03</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  countries  ISO                         region  hf_score  hf_rank  \\\n",
       "0  2019    Albania  ALB                 Eastern Europe      8.14     43.0   \n",
       "1  2019    Algeria  DZA     Middle East & North Africa      5.26    154.0   \n",
       "2  2019     Angola  AGO             Sub-Saharan Africa      6.09    129.0   \n",
       "3  2019  Argentina  ARG  Latin America & the Caribbean      7.38     74.0   \n",
       "4  2019    Armenia  ARM        Caucasus & Central Asia      8.20     40.0   \n",
       "\n",
       "   hf_quartile  pf_rol_procedural  pf_rol_civil  pf_rol_criminal  ...  \\\n",
       "0          2.0               5.97          4.76             4.26  ...   \n",
       "1          4.0               5.21          5.64             4.35  ...   \n",
       "2          4.0               2.72          4.43             3.60  ...   \n",
       "3          2.0               6.83          5.94             4.35  ...   \n",
       "4          1.0                NaN           NaN              NaN  ...   \n",
       "\n",
       "   ef_regulation_business_adm  ef_regulation_business_bureaucracy  \\\n",
       "0                        5.65                                6.67   \n",
       "1                        4.22                                2.22   \n",
       "2                        2.94                                2.44   \n",
       "3                        2.71                                5.78   \n",
       "4                        5.17                                5.56   \n",
       "\n",
       "   ef_regulation_business_start  ef_regulation_business_bribes  \\\n",
       "0                          9.74                           6.24   \n",
       "1                          9.31                           2.58   \n",
       "2                          8.73                           4.70   \n",
       "3                          9.58                           6.53   \n",
       "4                          9.86                           6.96   \n",
       "\n",
       "   ef_regulation_business_licensing  ef_regulation_business_compliance  \\\n",
       "0                              5.62                               7.18   \n",
       "1                              8.77                               7.03   \n",
       "2                              7.92                               6.78   \n",
       "3                              5.73                               6.51   \n",
       "4                              9.30                               7.04   \n",
       "\n",
       "   ef_regulation_business  ef_regulation  ef_score  ef_rank  \n",
       "0                    6.85           7.70      7.81     31.0  \n",
       "1                    5.69           5.84      4.90    162.0  \n",
       "2                    5.59           5.97      5.50    153.0  \n",
       "3                    6.14           5.99      5.50    153.0  \n",
       "4                    7.32           7.82      8.03     15.0  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We read data directly from the github csv file, and call the head to have a first look.\n",
    "\n",
    "df=pd.read_csv(\"https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff6dc58a90ecd286c8037bbd7e772466",
     "grade": true,
     "grade_id": "test1_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "I4Ai5L-bgQI-",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1ca96a1e0c052c3752e7d169fb898ed3",
     "grade": false,
     "grade_id": "cell-cac60b3d5a84bc9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-info\"><b>Exercise 2</b>\n",
    "\n",
    "First write the code to drop all the columns from the DataFrame ```df``` except ```['hf_quartile', 'ef_regulation',  'pf_expression', 'region']```, then drop all the rows from ```df``` containing missing values present in the selected columns.\n",
    "\n",
    "<br><i>[0.25 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Remember, Python is case-sensitive. The resulting DataFrame ```df``` should contain only four columns.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can drop all missing values using pandas ```.dropna()``` function. This step should normally be done \n",
    "after train-test-split, but being explicitly asked now, we'll do it outright. \n",
    "We also display the before and after code by running the cell to appreciate the changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e312a6da82f8c4bc37df98d9182e9662",
     "grade": false,
     "grade_id": "ex2",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before elimination!                                                                              pf_expression      0\n",
      "region             0\n",
      "ef_regulation    100\n",
      "hf_quartile      113\n",
      "dtype: int64\n",
      "\n",
      " Missing values after elimination!                                                                              hf_quartile      0\n",
      "ef_regulation    0\n",
      "pf_expression    0\n",
      "region           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# We directly call the columns needed from our dataset.\n",
    "\n",
    "df=df[['hf_quartile', 'ef_regulation',  'pf_expression', 'region']]\n",
    "\n",
    "# We drop the missing values, and check the state of NaN before and after elimination.\n",
    "\n",
    "print('Missing values before elimination!                                                                             ',\n",
    "      df.isna().sum().sort_values())\n",
    "\n",
    "df=df.dropna()\n",
    "\n",
    "print(\"\\n\", 'Missing values after elimination!                                                                             ',\n",
    "      df.isna().sum().sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "993b92c5f265ac6c55d3f4f81f079818",
     "grade": true,
     "grade_id": "test2_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "809062917104bc877969d8b047444ff1",
     "grade": false,
     "grade_id": "cell-c7c37aaa2f33b369",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 3</b> \n",
    "    \n",
    "Write the code to create the feature matrix ```X``` (```ef_regulation```,  ```pf_expression```, and ```region```) and the target array ```y``` (```hf_quartile```), then split them into separate training and test sets with the relative size of 0.75 and 0.25. Store the training and tests feature matrix in variables called ```X_train``` and ```X_test```, and the training and test label arrays as ```y_train``` and ```y_test```.\n",
    "    \n",
    "<br><i>[1 point]</i>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking the target variable type and distribution**\n",
    "\n",
    "In order to inform our train_test_split approach, we perform a simple exploratory data analysis\n",
    "for knowing target variable type (we'll see that being \"quartile\" a discrete float variable, we can\n",
    "consider it a classification task). We'll also analyze how many different values we have in the target\n",
    "feature (\"hf_quartile\") and its proportions, which seem to be balanced.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the types of our different features in our dataframe                                                      hf_quartile      float64\n",
      "ef_regulation    float64\n",
      "pf_expression    float64\n",
      "region            object\n",
      "dtype: object\n",
      "\n",
      " These are the different values distribution in the target variable                                             4.0    0.253883\n",
      "3.0    0.250134\n",
      "2.0    0.248527\n",
      "1.0    0.247456\n",
      "Name: hf_quartile, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"These are the types of our different features in our dataframe                                                     \",\n",
    "      df.dtypes)\n",
    "\n",
    "print('\\n', 'These are the different values distribution in the target variable                                            ', \n",
    "      df[\"hf_quartile\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating our target feature Series and our features matrix**\n",
    "\n",
    "We create target array and features matrix. We also perform the ```train_test_split```. We don't put a random_state parameter as we have not been instructed to do so, although it will be done in Exercise 7 to help make results repeatable downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "id": "rnxOTovpEqpu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69b315830f2239f6d81c0964227610ff",
     "grade": false,
     "grade_id": "ex3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We create target array and features marix.\n",
    "\n",
    "X=df.drop(\"hf_quartile\", axis=1)\n",
    "y=df[\"hf_quartile\"]\n",
    "\n",
    "# We perform the train-test-split with the specified parameters.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y,train_size=0.75, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a75ee900d85728951e8b462c9756e4a3",
     "grade": true,
     "grade_id": "test3_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af90ecf417b11a9c9a5c8f89ce72125e",
     "grade": true,
     "grade_id": "test3_2",
     "locked": true,
     "points": 0.75,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d0ed1b62e0021c85ec54b300514e03a",
     "grade": false,
     "grade_id": "cell-c7c37aaa2f33b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 4 </b> \n",
    "\n",
    "    \n",
    "The resulting feature matrix contains a categorical variable. Write the code to create a ```ColumnTransformer``` to encode it using the one-hot encoding method. Store the transformer in a variable called ```transformer```. At this stage, you do not need to run it.\n",
    "\n",
    "<br><i>[1 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "\n",
    "Not all the attributes are categorical. Ensure that all non-categorical attributes remain intact.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ColumnTransformer: OneHotEncoding for \"region\" variable**\n",
    "\n",
    "As we saw in the previous df.dtypes exploratory analysis, \"region\" feature is the categorical feature.\n",
    "We will transform it with a ```ColumnTransformer```, putting ```OneHotEncoder``` class as the transformation. We then use the parameter ```remainder=\"passthrough``` to leave the remaining X_train variables untouched. In order to call the index position of \"region\" inside ```X_train```, we used ```X_train.columns.get_loc(\"region\")```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00ef2091bcd0c6ccc4ed66e901f1fff9",
     "grade": false,
     "grade_id": "ex4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We import the necessary scikit-learn classes that we will use in the ColumnTransformer and the Pipeline.\n",
    "# These classes will also be used in Exercise 7.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the transformer with the ColumnTransformer, complying with the guidelines.\n",
    "\n",
    "transformer=ColumnTransformer([(\"ohe\", OneHotEncoder(sparse=False), [X_train.columns.get_loc(\"region\")])],\n",
    "                              remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27bbb525950b887058d2bde69862a60d",
     "grade": true,
     "grade_id": "test4_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "06fbab0612fd581dc99cc8b78ae50153",
     "grade": true,
     "grade_id": "test4_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f51c987424701b2b20389d302eda4dd4",
     "grade": true,
     "grade_id": "test4_3",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a3cdf9b3d150a148d5274132be6a8c3",
     "grade": false,
     "grade_id": "cell-c7c37a1aa2f33b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 5 </b> \n",
    "\n",
    "Write the code to create a ```Pipeline``` consisting of a ```SimpleImputer``` with the most frequent strategy, the previous transformer, a standard scaler, and a logistic regression model. Store the resulting pipeline in a variable called ```pipe```.\n",
    "    \n",
    "<br><i>[1.5 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "\n",
    "Be sure you apply the data transformations in the correct order.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate the class ```Pipeline``` by means of the creation of a list of steps that we will use as parameters. Then, automation with transformation in it is feasible. We understand that having dropped all NaN's, the SimpleImputer could be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb47640b2984de2eac9bddf8c8c891c6",
     "grade": false,
     "grade_id": "ex5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We create a list with the subsequent instantiation of the different classes needed in the pipeline.\n",
    "\n",
    "steps=[(\"Imputation\", SimpleImputer(strategy=\"most_frequent\")), \n",
    "       (\"Transformer\", transformer), \n",
    "       (\"Scaler\", StandardScaler()),\n",
    "       (\"Logistic Regression\", LogisticRegression())]\n",
    "\n",
    "# Lastly, we instantiate the pipeline with the steps.\n",
    "\n",
    "pipe=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "feab52ffd464f7779169dc62aeeef594",
     "grade": true,
     "grade_id": "test5_1",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7e6fd68043811c24be55eafc6454664",
     "grade": true,
     "grade_id": "test5_2",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32648c3253e28edaa2a268b2b7600b2c",
     "grade": true,
     "grade_id": "test5_3",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "26247cc3162479a2cd80163ca94d1788",
     "grade": true,
     "grade_id": "test5_4",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "82dcaea030fbb30c949fe3a4c9c4100d",
     "grade": true,
     "grade_id": "test5_5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cb719dff68f6fe54574bff8bafc24393",
     "grade": false,
     "grade_id": "cell-c7c37a1aa2fjk33b36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 6 </b> \n",
    "    \n",
    "Write the code to store the achieved ```score``` (accuracy) in a variable called ```score```. \n",
    "    \n",
    "<br><i>[1 point]</i>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "\n",
    "Use train and test datasets correctly.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train/fit the pipeline as if it was a normal ML model with our training data, and then we proceed to calculate the scores with our test data (scores are built-in inside the pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5d70d0859a7e9513d16031567e1eb90",
     "grade": false,
     "grade_id": "ex6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7644539614561028\n"
     ]
    }
   ],
   "source": [
    "# We fit the pipeline as if it was any other model, with the train data. We predict with our test data.\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.predict(X_test)\n",
    "\n",
    "# We output accuracy with test data.\n",
    "\n",
    "score= pipe.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d9cf0ab476714b541316a114608d967c",
     "grade": true,
     "grade_id": "test6_1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# LEAVE BLANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "evERSqB9MPid",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31cda44ca5d916698cf6df997f4aaee4",
     "grade": false,
     "grade_id": "cell-c7c37a1aa2fjk33bhj36",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-info\"><b>Exercise 7 </b> \n",
    "    \n",
    "The previous exercises were simple because they included only three columns. Now repeat the same process but using the complete dataset. This exercise is open. You can use any scaler, imputer, transformer, or encoder. The only requirement is to train a logistic regression. If you decide to drop a column, justify the reason. \n",
    "    \n",
    "<br><i>[5 points]</i>\n",
    "</div>\n",
    "\n",
    "<div class='alert alert-warning'>\n",
    "    \n",
    "The following columns are redundant and should be dropped:\n",
    "* ```year```\n",
    "* ```ISO```\n",
    "* ```countries```\n",
    "* All columns containing the word ```rank``` \n",
    "* All columns containing the word ```score```\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of the Exercise 7\n",
    "\n",
    "We have structured this exercise in seven tasks:\n",
    "\n",
    "1. Elimination of unique identifiers and redundant variables\n",
    "2. Multicollinearity: removal of features with Pearson > 90%\n",
    "3. Missing values in target feature: removal\n",
    "4. Missing values > 5%: feature removal\n",
    "5. Target and features data + train-test-split\n",
    "6. ColumnTransformer and Pipeline\n",
    "7. Amplify evaluation metrics: Confusion matrix + Precision and Recall\n",
    "\n",
    "All necessary classes already imported in previous exercises will not be re-stated if not new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "id": "rnxOTovpEqpu",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d63da504327f2eb69ca079359b186511",
     "grade": true,
     "grade_id": "ex7",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>countries</th>\n",
       "      <th>ISO</th>\n",
       "      <th>region</th>\n",
       "      <th>hf_score</th>\n",
       "      <th>hf_rank</th>\n",
       "      <th>hf_quartile</th>\n",
       "      <th>pf_rol_procedural</th>\n",
       "      <th>pf_rol_civil</th>\n",
       "      <th>pf_rol_criminal</th>\n",
       "      <th>...</th>\n",
       "      <th>ef_regulation_business_adm</th>\n",
       "      <th>ef_regulation_business_bureaucracy</th>\n",
       "      <th>ef_regulation_business_start</th>\n",
       "      <th>ef_regulation_business_bribes</th>\n",
       "      <th>ef_regulation_business_licensing</th>\n",
       "      <th>ef_regulation_business_compliance</th>\n",
       "      <th>ef_regulation_business</th>\n",
       "      <th>ef_regulation</th>\n",
       "      <th>ef_score</th>\n",
       "      <th>ef_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2019</td>\n",
       "      <td>Albania</td>\n",
       "      <td>ALB</td>\n",
       "      <td>Eastern Europe</td>\n",
       "      <td>8.14</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.97</td>\n",
       "      <td>4.76</td>\n",
       "      <td>4.26</td>\n",
       "      <td>...</td>\n",
       "      <td>5.65</td>\n",
       "      <td>6.67</td>\n",
       "      <td>9.74</td>\n",
       "      <td>6.24</td>\n",
       "      <td>5.62</td>\n",
       "      <td>7.18</td>\n",
       "      <td>6.85</td>\n",
       "      <td>7.70</td>\n",
       "      <td>7.81</td>\n",
       "      <td>31.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>DZA</td>\n",
       "      <td>Middle East &amp; North Africa</td>\n",
       "      <td>5.26</td>\n",
       "      <td>154.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.21</td>\n",
       "      <td>5.64</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.22</td>\n",
       "      <td>9.31</td>\n",
       "      <td>2.58</td>\n",
       "      <td>8.77</td>\n",
       "      <td>7.03</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.84</td>\n",
       "      <td>4.90</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019</td>\n",
       "      <td>Angola</td>\n",
       "      <td>AGO</td>\n",
       "      <td>Sub-Saharan Africa</td>\n",
       "      <td>6.09</td>\n",
       "      <td>129.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.72</td>\n",
       "      <td>4.43</td>\n",
       "      <td>3.60</td>\n",
       "      <td>...</td>\n",
       "      <td>2.94</td>\n",
       "      <td>2.44</td>\n",
       "      <td>8.73</td>\n",
       "      <td>4.70</td>\n",
       "      <td>7.92</td>\n",
       "      <td>6.78</td>\n",
       "      <td>5.59</td>\n",
       "      <td>5.97</td>\n",
       "      <td>5.50</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2019</td>\n",
       "      <td>Argentina</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Latin America &amp; the Caribbean</td>\n",
       "      <td>7.38</td>\n",
       "      <td>74.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.83</td>\n",
       "      <td>5.94</td>\n",
       "      <td>4.35</td>\n",
       "      <td>...</td>\n",
       "      <td>2.71</td>\n",
       "      <td>5.78</td>\n",
       "      <td>9.58</td>\n",
       "      <td>6.53</td>\n",
       "      <td>5.73</td>\n",
       "      <td>6.51</td>\n",
       "      <td>6.14</td>\n",
       "      <td>5.99</td>\n",
       "      <td>5.50</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2019</td>\n",
       "      <td>Armenia</td>\n",
       "      <td>ARM</td>\n",
       "      <td>Caucasus &amp; Central Asia</td>\n",
       "      <td>8.20</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5.17</td>\n",
       "      <td>5.56</td>\n",
       "      <td>9.86</td>\n",
       "      <td>6.96</td>\n",
       "      <td>9.30</td>\n",
       "      <td>7.04</td>\n",
       "      <td>7.32</td>\n",
       "      <td>7.82</td>\n",
       "      <td>8.03</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  countries  ISO                         region  hf_score  hf_rank  \\\n",
       "0  2019    Albania  ALB                 Eastern Europe      8.14     43.0   \n",
       "1  2019    Algeria  DZA     Middle East & North Africa      5.26    154.0   \n",
       "2  2019     Angola  AGO             Sub-Saharan Africa      6.09    129.0   \n",
       "3  2019  Argentina  ARG  Latin America & the Caribbean      7.38     74.0   \n",
       "4  2019    Armenia  ARM        Caucasus & Central Asia      8.20     40.0   \n",
       "\n",
       "   hf_quartile  pf_rol_procedural  pf_rol_civil  pf_rol_criminal  ...  \\\n",
       "0          2.0               5.97          4.76             4.26  ...   \n",
       "1          4.0               5.21          5.64             4.35  ...   \n",
       "2          4.0               2.72          4.43             3.60  ...   \n",
       "3          2.0               6.83          5.94             4.35  ...   \n",
       "4          1.0                NaN           NaN              NaN  ...   \n",
       "\n",
       "   ef_regulation_business_adm  ef_regulation_business_bureaucracy  \\\n",
       "0                        5.65                                6.67   \n",
       "1                        4.22                                2.22   \n",
       "2                        2.94                                2.44   \n",
       "3                        2.71                                5.78   \n",
       "4                        5.17                                5.56   \n",
       "\n",
       "   ef_regulation_business_start  ef_regulation_business_bribes  \\\n",
       "0                          9.74                           6.24   \n",
       "1                          9.31                           2.58   \n",
       "2                          8.73                           4.70   \n",
       "3                          9.58                           6.53   \n",
       "4                          9.86                           6.96   \n",
       "\n",
       "   ef_regulation_business_licensing  ef_regulation_business_compliance  \\\n",
       "0                              5.62                               7.18   \n",
       "1                              8.77                               7.03   \n",
       "2                              7.92                               6.78   \n",
       "3                              5.73                               6.51   \n",
       "4                              9.30                               7.04   \n",
       "\n",
       "   ef_regulation_business  ef_regulation  ef_score  ef_rank  \n",
       "0                    6.85           7.70      7.81     31.0  \n",
       "1                    5.69           5.84      4.90    162.0  \n",
       "2                    5.59           5.97      5.50    153.0  \n",
       "3                    6.14           5.99      5.50    153.0  \n",
       "4                    7.32           7.82      8.03     15.0  \n",
       "\n",
       "[5 rows x 125 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We call again our whole dataframe to perform needed operations from scratch.\n",
    "\n",
    "df=pd.read_csv(\"https://github.com/jnin/information-systems/raw/main/data/hfi_cc_2021.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Elimination of unique identifiers and redundant variables ###\n",
    "\n",
    "We eliminate the aforementioned variables from the original dataframe, as they are either unique identifiers without predictive power or redundant in terms of information. By such operations, we eliminated 11 features and are left with 114."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are left with 114 features from the original 125.\n"
     ]
    }
   ],
   "source": [
    "df.drop([\"year\", \"ISO\", \"countries\"], axis=1, inplace=True)\n",
    "df.drop(list(df.filter(regex = 'rank')), axis = 1, inplace = True)\n",
    "df.drop(list(df.filter(regex = 'score')), axis = 1, inplace = True)\n",
    "\n",
    "print(f'We are left with {df.shape[1]} features from the original 125.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multicollinearity: removal of features with Pearson > 90%\n",
    "\n",
    "Secondly, we will check for multicollinearity. In order to simplify our analysis, we'll temporarily drop \n",
    "\"region\", the only non-numerical variable. Then, given the difficulty of plotting a Seaborn heatmap \n",
    "with 114 columns, we will proceed to use 0.9 as a correlation threshold to drop features already \n",
    "explained by others through detailed code. We use 0.9 because we want to follow a conservative approach and not drop features that may have good predictive power. We end up dropping 43 variables, and are left with 71."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop first temporarily \"region\" to simplify our correlation analysis for multicollinearity spotting.\n",
    "\n",
    "df1=df.drop(\"region\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_womens                          pf_identity                          0.98\n",
      "pf_identity                        pf_womens                            0.98\n",
      "pf_identity_inheritance            pf_identity_inheritance_daughters    0.98\n",
      "                                   pf_identity_inheritance_widows       0.98\n",
      "pf_identity_inheritance_daughters  pf_identity_inheritance              0.98\n",
      "                                                                        ... \n",
      "pf_rol                             ef_legal_courts                      0.91\n",
      "pf_expression                      pf_expression_cultural               0.91\n",
      "                                   pf_assembly                          0.91\n",
      "                                   pf_expression_media                  0.91\n",
      "pf_expression_cultural             pf_expression                        0.91\n",
      "Length: 70, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#We create the correlation matrix numerically without displaying it.\n",
    "\n",
    "corr_matrix=df1.corr().round(2)\n",
    "\n",
    "correlated_pairs = corr_matrix.abs().unstack().sort_values(ascending=False)\n",
    "\n",
    "# get the pairs with correlation greater than a threshold (here, 0.9).\n",
    "threshold = 0.9\n",
    "correlated_pairs = correlated_pairs[correlated_pairs > threshold]\n",
    "\n",
    "# drop the diagonal values, with perfect correlation with the own variable.\n",
    "correlated_pairs = correlated_pairs[correlated_pairs != 1]\n",
    "\n",
    "# display the correlated pairs\n",
    "print(correlated_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are correlated variables with Pearson >90%, having 43 variables: \n",
      " ['ef_trade_regulatory', 'pf_movement', 'pf_assembly_parties_bans', 'pf_expression_cultural', 'pf_assembly_parties_barriers', 'pf_identity_same', 'pf_assembly_freedom_cld', 'pf_assembly', 'ef_trade_regulatory_compliance', 'pf_expression_freedom_bti', 'pf_expression_media', 'pf_expression', 'pf_identity_inheritance_daughters', 'pf_rol_civil', 'pf_identity', 'pf_assembly_freedom', 'pf_rol_procedural', 'pf_rol_criminal', 'pf_identity_inheritance', 'pf_assembly_freedom_house', 'ef_government_tax_payroll', 'pf_movement_cld', 'ef_government_tax', 'pf_expression_freedom', 'pf_identity_same_m', 'pf_assembly_freedom_bti', 'pf_religion_freedom_vdem', 'pf_religion_suppression', 'pf_religion_freedom', 'pf_assembly_civil', 'pf_assembly_parties', 'pf_expression_freedom_cld', 'pf_religion', 'pf_identity_inheritance_widows', 'pf_movement_vdem', 'pf_identity_same_f', 'pf_movement_vdem_foreign', 'pf_assembly_parties_auton', 'pf_religion_freedom_cld', 'pf_rol', 'pf_assembly_entry', 'ef_legal_courts', 'pf_womens']\n"
     ]
    }
   ],
   "source": [
    "# Then, we treat this Series Index as a list, join them together and take out only those features that are\n",
    "# unique, which can be carried out with the set() method.\n",
    "\n",
    "correlated_variables=list(correlated_pairs.index)\n",
    "correlated_variables\n",
    "\n",
    "variables_repeated=[]\n",
    "for pair in correlated_variables:\n",
    "    variables_repeated.append(pair[0])\n",
    "    variables_repeated.append(pair[1])\n",
    "\n",
    "multicollinearity=list(set(variables_repeated))\n",
    "\n",
    "print('These are correlated variables with Pearson >90%, having {} variables:'.format(len(multicollinearity)),\n",
    "      \"\\n\",multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We drop 43 variables, ending up with 71.\n"
     ]
    }
   ],
   "source": [
    "#We drop those 43 features that affect multicollinearity in the main DataFrame.\n",
    "\n",
    "df.drop(multicollinearity, axis=1, inplace=True)\n",
    "\n",
    "print(f'We drop {len(multicollinearity)} variables, ending up with {df.shape[1]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Missing Values in target Feature: removal\n",
    "\n",
    "Logistic Regression does not accept missing values, and this holds true for the target feature. We could run a NaN analysis after ```train-test-split``` and either eliminate them or impute them inside a ```Pipeline```, but the target feature NaN's would trigger an error (as we don't touch them).\n",
    "\n",
    "Therefore, as imputation in the target feature would not make any predictive sense (as we would alter the estimation of reality), we will first remove them before the split. We'll do it specifying the \"subset\" parameter, dropping 113 instances and finishing up with 1867."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total, we have dropped 113 instances by removing target feature NaNs.\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"hf_quartile\"], inplace=True)\n",
    "\n",
    "print(f'In total, we have dropped {1980-df.shape[0]} instances by removing target feature NaNs.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Missing Values > 5%: feature removal\n",
    "\n",
    "We will now proceed to impute different missing values. We follow two approaches:\n",
    "\n",
    "1. If missing data in a feature is >5% of all instances, we remove the feature. We do this to prevent additional noise.\n",
    "2. If missing data in a feature is <5%, we replace with the median on numerical features, and with mode on categorical features. This step will be carried out after train_test_split.\n",
    "\n",
    "We explore first the degree of missing values by feature, eliminate features with >5% of NaN, and finally create the imputers. At he end, we drop 5 columns and we are left with 65 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>columns</th>\n",
       "      <th>missing</th>\n",
       "      <th>proportionnmiss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>ef_legal_military</td>\n",
       "      <td>175</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>ef_regulation_business_adm</td>\n",
       "      <td>135</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>ef_regulation_labor_firing</td>\n",
       "      <td>137</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>pf_ss_disappearances_organized</td>\n",
       "      <td>134</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>ef_regulation_labor_bargain</td>\n",
       "      <td>136</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>ef_money_sd</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>ef_money_inflation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>ef_money_currency</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>ef_money</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>ef_regulation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           columns  missing  proportionnmiss\n",
       "30               ef_legal_military      175             0.09\n",
       "63      ef_regulation_business_adm      135             0.07\n",
       "57      ef_regulation_labor_firing      137             0.07\n",
       "5   pf_ss_disappearances_organized      134             0.07\n",
       "58     ef_regulation_labor_bargain      136             0.07\n",
       "..                             ...      ...              ...\n",
       "37                     ef_money_sd        0             0.00\n",
       "38              ef_money_inflation        0             0.00\n",
       "39               ef_money_currency        0             0.00\n",
       "40                        ef_money        0             0.00\n",
       "70                   ef_regulation        0             0.00\n",
       "\n",
       "[71 rows x 3 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We first create a DataFrame displaying the number of missing values and proportions per column.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "columns= list(df.columns)\n",
    "missing=list(df.isna().sum())\n",
    "proportionmiss=list((np.array(df.isna().sum())/1867).round(2))\n",
    "\n",
    "missing_df=pd.DataFrame({\"columns\": columns, \"missing\": missing, \"proportionnmiss\": proportionmiss})\n",
    "missing_df.sort_values(\"proportionnmiss\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           columns  missing  proportionnmiss\n",
      "5   pf_ss_disappearances_organized      134             0.07\n",
      "30               ef_legal_military      175             0.09\n",
      "52  ef_regulation_credit_ownership      105             0.06\n",
      "57      ef_regulation_labor_firing      137             0.07\n",
      "58     ef_regulation_labor_bargain      136             0.07\n",
      "63      ef_regulation_business_adm      135             0.07\n",
      "\n",
      " In total, we have dropped 6 columns, ending up with 65 features.\n"
     ]
    }
   ],
   "source": [
    "# We have 6 variables that surpass the >5% of NaN threshold. We remove them from the DataFrame.\n",
    "\n",
    "print(missing_df[missing_df[\"proportionnmiss\"]>0.05])\n",
    "\n",
    "missing_5=list(missing_df[missing_df[\"proportionnmiss\"]>0.05][\"columns\"])\n",
    "df.drop(missing_5, axis=1, inplace=True)\n",
    "\n",
    "print(\"\\n\", f'In total, we have dropped {len(missing_5)} columns, ending up with {df.shape[1]} features.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5. Target and features data + train_test_split\n",
    "\n",
    "Up to here, we have the final data to input to our model. \n",
    "\n",
    "**Columns**: All highly correlated variables from a 0.9 of Pearson score are removed, and columns with NaN> 5% are also out, being left with 65 features.\n",
    "\n",
    "**Instances**: First removal of target feature NaN's, as the final pipeline would output an error in the score.\n",
    "\n",
    "We will now create our target array and features matrix, performing the ```train_test_split``` with a ```random_state``` to make our results repeatable downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our target array (y) and our features matrix (X) with a random_state.\n",
    "\n",
    "X=df.drop(\"hf_quartile\", axis=1)\n",
    "y=df[\"hf_quartile\"]\n",
    "\n",
    "# We then proceed to split our data.\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ColumnTransformer and Pipeline ###\n",
    "\n",
    "We will now set a column transformation strategy for each different category. The strategy for each type is:\n",
    "\n",
    "1. Categorical features (\"region\"): We OneHotEncode the variable, no imputation needed as no missing values are present.\n",
    "2. Numerical features: we replace missing values by median, with less sensitivity to extreme values tthan mean.\n",
    "\n",
    "We will create for each typology the ColumnTransformer, and impute it inside the final pipeline that will add a scaler (as logistic regression is sensible in terms of distance) and the ```LogisticRegression``` model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "float64    63\n",
       "object      1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, we ensure that \"region\" is the only categorical variable in our train DataFrame. It is.\n",
    "\n",
    "X_train.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create he ColumnTransformer with the aforementioned strategies for categorical and numerical values.\n",
    "# We call by index positions the different columns, as strings are not supported for NumPy arrays.\n",
    "\n",
    "imputer=SimpleImputer(strategy=\"median\")\n",
    "onehot=OneHotEncoder(sparse=False)\n",
    "\n",
    "transformer=ColumnTransformer([(\"encode\", onehot, [X_train.columns.get_loc(\"region\")]), \n",
    "                              (\"medianimpute\", imputer, list(range(1,64)))]\n",
    "                              ,remainder=\"passthrough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create the pipeline.\n",
    "\n",
    "scaler=StandardScaler()\n",
    "logreg=LogisticRegression()\n",
    "\n",
    "steps=[(\"transformer\", transformer),\n",
    "       (\"scaler\", scaler),\n",
    "       (\"logreg\", logreg)]\n",
    "\n",
    "pipe=Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9100642398286938"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We fit the model to our training data and calculate the accuracy score.\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Amplifying evaluation metrics: Confusion matrix + Precision and Recall ###\n",
    "\n",
    "**The accuracy score of 91% shows an increase regarding the previous dataset (approximately, of +15%). Those are good news, but we can actually further breakdown prediction metrics by each of the four quartiles precision and recall.**\n",
    "\n",
    "We will see that quartile 2 shows the worst general metrics, although overall, quality of predictions (precision) and sensitivity of predictions (recall) seem good, being always superior to 85%.\n",
    "\n",
    "***Given such metrics, we assume the exercise can be concluded, at the expense of waiting for the business objective for fully determining if metrics are good enough or not.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the necessary modules for further evaluation.\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[110  10   0   0]\n",
      " [ 11 100   3   0]\n",
      " [  0   8 104   6]\n",
      " [  0   0   4 111]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.91      0.92      0.91       120\n",
      "         2.0       0.85      0.88      0.86       114\n",
      "         3.0       0.94      0.88      0.91       118\n",
      "         4.0       0.95      0.97      0.96       115\n",
      "\n",
      "    accuracy                           0.91       467\n",
      "   macro avg       0.91      0.91      0.91       467\n",
      "weighted avg       0.91      0.91      0.91       467\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We predict the instances in order to make a comparison with already established test features.\n",
    "\n",
    "y_pred=pipe.predict(X_test)\n",
    "\n",
    "# Evaluation metrics broken down by different categories, displaying also the confusion matrix.\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\n\", classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session I.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
